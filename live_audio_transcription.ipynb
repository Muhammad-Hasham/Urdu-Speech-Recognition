{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HPxIC8-OepP"
      },
      "outputs": [],
      "source": [
        "#Muhammad Hasham Ul Haq\n",
        "#i200752@nu.edu.pk\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "import IPython\n",
        "from torchaudio.models.wav2vec2 import Wav2Vec2ForCTC\n",
        "from torchaudio.transforms import Resample\n",
        "from tortoise.api import TextToSpeech\n",
        "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
        "\n",
        "# Parameters for audio recording\n",
        "sample_rate = 16000\n",
        "audio_buffer = []\n",
        "buffer_duration = 2  # 2-second buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Wav2Vec2 model for audio transcription\n",
        "model, _ = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "\n",
        "# Initialize Tortoise TTS for voice cloning\n",
        "tts = TextToSpeech()\n"
      ],
      "metadata": {
        "id": "INo40-UDOunP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle incoming audio chunks\n",
        "def audio_callback(indata, frames, time, status):\n",
        "    if status:\n",
        "        print(\"Error:\", status)\n",
        "    if any(indata):\n",
        "        audio_buffer.extend(indata)\n",
        "        if len(audio_buffer) > sample_rate * buffer_duration:\n",
        "            process_audio()\n",
        "\n",
        "# Function to transcribe and speak audio\n",
        "def process_audio():\n",
        "    audio_data = torch.FloatTensor(audio_buffer)\n",
        "    audio_buffer.clear()\n",
        "\n",
        "    # Transcribe audio\n",
        "    audio_data = resampler(audio_data)\n",
        "    with torch.no_grad():\n",
        "        input_values = model.feature_extractor(audio_data.unsqueeze(0))\n",
        "        logits = model.classifier(input_values)\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = model.tokenizer.batch_decode(predicted_ids)\n",
        "\n",
        "    print(\"Transcription:\", transcription)\n",
        "\n",
        "    # Convert transcription to voice (you can choose a voice from the available ones)\n",
        "    voice = 'voice_test'\n",
        "    text = transcription[0]  # Use the transcription as text\n",
        "    voice_samples, conditioning_latents = load_voice(voice)\n",
        "    gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=\"standard\")\n",
        "    torchaudio.save('generated.wav', gen.squeeze(0).cpu(), 24000)\n",
        "    IPython.display.Audio('generated.wav')\n",
        "\n",
        "# Start recording live audio\n",
        "with sd.InputStream(callback=audio_callback, channels=1, samplerate=sample_rate):\n",
        "    print(\"Listening for audio...\")\n",
        "    sd.sleep(1000000)\n"
      ],
      "metadata": {
        "id": "vOoECk_fOxQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ToxpmzEvOyXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}