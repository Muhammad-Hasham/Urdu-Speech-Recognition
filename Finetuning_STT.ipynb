{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHpSnefSFUqf"
      },
      "outputs": [],
      "source": [
        "#Muhammad Hasham Ul Haq\n",
        "#i200752@nu.edu.pk\n",
        "%pip install transformers pandas numpy librosa datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtEOJQWdGv5l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import librosa\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer, Wav2Vec2CTCTokenizer\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "vocab_list = {\"ن\": 0, \"ر\": 1, \"ٸ\": 2, \"f\": 3, \"b\": 4, \"l\": 6, \"ح\": 7, \"خ\": 8, \"ع\": 9, \"ش\": 10, \"س\": 11, \"0\": 12, \"إ\": 13, \"ف\": 14, \"و\": 15, \"ی\": 16, \"۱\": 17, \"‌\": 18, \"ۃ\": 19, \"i\": 20, \"ڑ\": 21, \"ل\": 22, \"2\": 23, \"3\": 24, \"۰\": 25, \"ۓ\": 26, \"۲\": 27, \"a\": 28, \"ک\": 29, \"‬\": 30, \"ت\": 31, \"v\": 32, \"d\": 33, \"ﷲ\": 34, \"۵\": 35, \"r\": 36, \"‫\": 37, \"h\": 38, \"پ\": 39, \"ژ\": 40, \"w\": 41, \"‎\": 42, \"‍\": 43, \"u\": 44, \"s\": 45, \"ب\": 46, \"ث\": 47, \"1\": 48, \"ض\": 49, \"۶\": 50, \"گ\": 51, \"ۂ\": 52, \"ں\": 53, \"ك\": 54, \"ط\": 55, \"t\": 56, \"ھ\": 57, \"5\": 58, \"7\": 59, \"غ\": 60, \"ـ\": 61, \"ء\": 62, \"ظ\": 63, \"ي\": 64, \"8\": 65, \"c\": 66, \"⁦\": 67, \"ص\": 68, \"e\": 69, \"­\": 70, \"۸\": 71, \"۳\": 72, \"6\": 73, \"m\": 74, \"ٹ\": 75, \"چ\": 76, \"آ\": 77, \"ﮟ\": 78, \"ا\": 79, \"‏\": 80, \"ئ\": 81, \"g\": 82, \"o\": 83, \"۷\": 84, \" \": 85, \"4\": 86, \"۴\": 87, \"د\": 88, \"​\": 89, \"ق\": 90, \"ڈ\": 91, \"k\": 92, \"p\": 93, \"ذ\": 94, \"ج\": 95, \"ؤ\": 96, \"ہ\": 97, \"ز\": 98, \"n\": 99, \"`\": 100, \"x\": 101, \"۹\": 102, \"ه\": 103, \"ے\": 104, \"9\": 105, \"م\": 106, \"|\": 5, \"[UNK]\": 107, \"[PAD]\": 108}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHgMlqMBGzOc"
      },
      "outputs": [],
      "source": [
        "# Load your CSV file\n",
        "data = pd.read_csv('cleaned_file.csv')  # Replace with your CSV file path\n",
        "\n",
        "# Preprocess transcripts if needed\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "data['transcript'] = data['transcript'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a8LczdqHCPn"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Create a processor using your vocabulary\n",
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}  # Assume vocab_list is already defined\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "# Load the audio files and preprocess transcripts\n",
        "def load_audio(file_name):\n",
        "    file_path = f'/content/drive/MyDrive/UrduRecordings.zip (Unzipped Files)/Train/{file_name}'  # Update with the correct path\n",
        "    audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
        "    return audio\n",
        "\n",
        "data['audio'] = data['audio_File'].apply(load_audio)\n",
        "data = data[['transcript', 'audio']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhZ4dkhmHGDA"
      },
      "outputs": [],
      "source": [
        "# Convert DataFrame to a Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb7G1i7vHGv1"
      },
      "outputs": [],
      "source": [
        "# Define the preparation function\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"transcript\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
        "\n",
        "# Define the data collator\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG6qC9LkHJOx"
      },
      "outputs": [],
      "source": [
        "# Split the dataset if needed\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8hW-Ty1HMs8"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\",\n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")\n",
        "\n",
        "# Freeze the feature encoder\n",
        "model.freeze_feature_encoder()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSjfjUzoHNsj"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./urdu_transefer_learning\",\n",
        "    group_by_length=True,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=30,\n",
        "    fp16=True,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=3e-4,\n",
        "    warmup_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11cInc9UHR5U"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK5TqgYcQOcz"
      },
      "outputs": [],
      "source": [
        "\n",
        "#code for quantization of the model\n",
        "trained_model.eval()\n",
        "quantized_model = quantize_dynamic(\n",
        "    trained_model,\n",
        "    {torch.nn.LSTM, torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model.state_dict(), 'quantized_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
